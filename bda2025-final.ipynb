{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "446f837b-5f3e-4941-b98c-c945cea04d33",
    "_uuid": "56ed36f8-59a6-43be-9687-267ae7b1a945",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "# BDA2025 Final Project - Clustering\n",
    "\n",
    "This notebook is for BDA2025 Final Project, focusing on the usage of clustering method. The task is to analyze the relationships within this dataset and **classify the data into 4n – 1 clusters**, where n is the number of dimensions of the data.\n",
    "\n",
    "The results will finally be evaluated based on the Fowlkes–Mallows Index (FMI), which measures the similarity between your clustering results and a hidden ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "08f1b619-393a-4f4d-8885-6d601bf09e25",
    "_uuid": "c71d8580-3bc3-422a-b4b9-116a6287a928",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d7cb3ed1-3f75-4a4a-a68a-09b1dbb99d67",
    "_uuid": "93fde4df-b738-435f-adad-b9baa9bf9c60",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer, QuantileTransformer\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from scipy.spatial import distance\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time \n",
    "import re\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3cab7ba4-5df8-493c-8dee-8bb1589f9def",
    "_uuid": "ee3568e0-5ebb-4158-81d2-3766f85a8a8d",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Clustering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c39ff711-5fe5-4d21-affb-6ee42ebad74c",
    "_uuid": "abe1f6df-7cac-43cb-aa71-993ff0a39e1c",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def perform_clustering(input_file, output_file,\n",
    "                       method='specialized_gmm',\n",
    "                       scaler_type='robust',\n",
    "                       covariance_type='tied',\n",
    "                       adjacent_focus=None,\n",
    "                       flags=\"\"\n",
    "                      ):\n",
    "    \"\"\"Advanced clustering for physics event data with exponential-like distribution.\"\"\"\n",
    "    \n",
    "    # Load the dataset\n",
    "    data_df = pd.read_csv(input_file)\n",
    "\n",
    "    # Identify ID column\n",
    "    id_col = data_df.columns[0]\n",
    "    if id_col.lower() == 'id':\n",
    "        data_df.rename(columns={id_col: 'id'}, inplace=True)\n",
    "    else:\n",
    "        print(f\"[Warning] No 'id' column found. Using first column '{id_col}' as ID\")\n",
    "        data_df.rename(columns={id_col: 'id'}, inplace=True)\n",
    "\n",
    "    output_ids = data_df['id']\n",
    "    features_df = data_df.drop(columns=['id'])\n",
    "    n_dimensions = features_df.shape[1]\n",
    "    \n",
    "    print(f\"Dataset shape: {features_df.shape}\")\n",
    "    print(f\"Data distribution summary:\")\n",
    "    print(features_df.describe())\n",
    "    \n",
    "    # Choose number of clusters based on method and hint\n",
    "    k_clusters = 4 * n_dimensions - 1\n",
    "    \n",
    "    print(f\"Target clusters: {k_clusters}\")\n",
    "    if adjacent_focus:\n",
    "        print(f\"Focusing on adjacent dimensions: {adjacent_focus}\")\n",
    "\n",
    "    # 1. (Optional) Specialized Feature Engineering for Physics Data\n",
    "    if \"feature\" in flags:\n",
    "        print(\"[FEATURE] Feature engineering ENABLED.\")\n",
    "        processed_features = engineer_physics_features(features_df, method)\n",
    "    else:\n",
    "        processed_features = features_df\n",
    "        \n",
    "    # 2. Specialized Scaling for Exponential-like Distribution\n",
    "    scaled_features = apply_specialized_scaling(processed_features, scaler_type)\n",
    "    print(f\"Features scaled using specialized {scaler_type} approach.\")\n",
    "    \n",
    "    # 3. (Optional) Outlier Handling - mark and potentially weight differently\n",
    "    # Note: You must modify other parts of the code to support this.\n",
    "    if \"outlier\" in flags:\n",
    "        print(\"[OUTLIER] Outlier handling ENABLED (not yet implemented).\")\n",
    "        # outlier_weights = detect_and_weight_outliers(scaled_features)\n",
    "\n",
    "    # 4. Dimensionality and Focus Strategy\n",
    "    if method == 'adjacent_focus':\n",
    "        # Focus on specified adjacent dimensions\n",
    "        final_features = focus_on_adjacent_dims(scaled_features, features_df.columns, adjacent_focus)\n",
    "    elif method == 'hybrid':\n",
    "        # Combine original features with adjacent dimension emphasis\n",
    "        final_features = create_hybrid_features(scaled_features, features_df.columns, adjacent_focus)\n",
    "    else:\n",
    "        final_features = scaled_features\n",
    "    \n",
    "    # 5. Specialized Clustering\n",
    "    print(f\"Fitting {method.upper()} model...\")\n",
    "\n",
    "    if method == 'hierarchical':\n",
    "        cluster_labels = perform_hierarchical_clustering(k_clusters, adjacent_focus, final_features)\n",
    "        model = None\n",
    "    else:\n",
    "        if method == 'hybrid':\n",
    "            model = create_hybrid_model(k_clusters, covariance_type)\n",
    "        elif method == 'adjacent_focus':\n",
    "            model = create_adjacent_focused_model(k_clusters, covariance_type)\n",
    "        elif method == 'specialized_gmm':\n",
    "            model = create_specialized_gmm(k_clusters, covariance_type, final_features)\n",
    "        elif method == 'specialized_gmm_v0':\n",
    "            model = create_specialized_gmm_v0(k_clusters, covariance_type, final_features)\n",
    "        elif method == 'gmm':\n",
    "            model = create_gmm_model(k_clusters, covariance_type)\n",
    "        else:\n",
    "            model = create_kmeans_model(k_clusters)\n",
    "\n",
    "        # Fit model\n",
    "        model.fit(final_features)\n",
    "        cluster_labels = model.predict(final_features)\n",
    "\n",
    "    # 6. (Optional) Post-processing: Refine clusters based on physics intuition\n",
    "    if \"refine\" in flags:\n",
    "        print(\"[REFINE] Physics-based refinement ENABLED.\")\n",
    "        cluster_labels = refine_clusters_physics_aware(cluster_labels, features_df)\n",
    "\n",
    "    # 7. Create submission\n",
    "    output_df = pd.DataFrame({\n",
    "        'id': output_ids,\n",
    "        'label': cluster_labels\n",
    "    })\n",
    "    output_df.to_csv(output_file, index=False)\n",
    "    print(f\"Submission saved to '{output_file}'\")\n",
    "    print(f\"Cluster distribution:\\n{output_df['label'].value_counts().sort_index()}\")\n",
    "    \n",
    "    return model, final_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c65f8383-cb50-40a9-b329-d7d2777e5378",
    "_uuid": "1453e870-1a39-4a23-9648-90fc83c1332a",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### 1. Pre-processing - Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4d262c1c-6989-4282-b2e1-ff89b17ce022",
    "_uuid": "e37f1691-aec6-49a0-b640-0e819b62ed2f",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def engineer_physics_features(features_df, method):\n",
    "    \"\"\"Create physics-aware features\"\"\"\n",
    "    \n",
    "    features = features_df.copy()\n",
    "    \n",
    "    if method in ['specialized_gmm', 'specialized_gmm_v0', 'hybrid', 'hierarchical']:\n",
    "        # Add magnitude/energy-like features\n",
    "        features['magnitude'] = np.sqrt((features ** 2).sum(axis=1))\n",
    "        \n",
    "        # Add ratios for all adjacent pairs (common in physics)\n",
    "        col_names = features_df.columns\n",
    "        for i in range(len(col_names) - 1):\n",
    "            col1, col2 = col_names[i], col_names[i+1]\n",
    "            features[f'{col2}_to_{col1}_ratio'] = np.log1p(features_df[col2]) - np.log1p(features_df[col1])\n",
    "        \n",
    "        # Add log-transformed features for exponential-like data\n",
    "        for col in features_df.columns:\n",
    "            features[f'log_{col}'] = np.log1p(features_df[col])\n",
    "            \n",
    "        # Add squared features (energy-like)\n",
    "        for col in features_df.columns:\n",
    "            features[f'sq_{col}'] = features_df[col] ** 2\n",
    "    \n",
    "    return features\n",
    "\n",
    "def apply_specialized_scaling(features, scaler_type):\n",
    "    \"\"\"Apply scaling optimized for physics data with exponential distribution\"\"\"\n",
    "    \n",
    "    if scaler_type == 'quantile':\n",
    "        # QuantileTransformer works well for non-normal distributions\n",
    "        scaler = QuantileTransformer(n_quantiles=min(1000, len(features)//2), \n",
    "                                   output_distribution='uniform', random_state=42)\n",
    "    elif scaler_type == 'power':\n",
    "        # PowerTransformer for making data more Gaussian\n",
    "        scaler = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "    elif scaler_type == 'robust':\n",
    "        scaler = RobustScaler()\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "    \n",
    "    try:\n",
    "        scaled_features = scaler.fit_transform(features)\n",
    "        return scaled_features\n",
    "    except:\n",
    "        # Fallback to robust scaling if specialized scaling fails\n",
    "        print(\"Falling back to RobustScaler\")\n",
    "        return RobustScaler().fit_transform(features)\n",
    "\n",
    "def detect_and_weight_outliers(features):\n",
    "    \"\"\"Detect outliers and create weights\"\"\"\n",
    "    outlier_detector = IsolationForest(contamination=0.05, random_state=42)\n",
    "    outlier_labels = outlier_detector.fit_predict(features)\n",
    "    \n",
    "    # Create weights: lower weight for outliers\n",
    "    weights = np.ones(len(features))\n",
    "    weights[outlier_labels == -1] = 0.5\n",
    "    \n",
    "    print(f\"Detected {(outlier_labels == -1).sum()} outliers ({(outlier_labels == -1).mean():.1%})\")\n",
    "    return weights\n",
    "\n",
    "def focus_on_adjacent_dims(scaled_features, column_names, adjacent_focus):\n",
    "    \"\"\"Focus on specified adjacent dimensions\"\"\"\n",
    "    \n",
    "    # Extract dimension indices from focus string (e.g., 'dim23' -> indices 1,2)\n",
    "    dim_numbers = [int(c) for c in adjacent_focus if c.isdigit()]\n",
    "    if len(dim_numbers) >= 2:\n",
    "        dim1_idx = dim_numbers[0] - 1  # Convert to 0-indexed\n",
    "        dim2_idx = dim_numbers[1] - 1\n",
    "        \n",
    "        # Ensure indices are valid\n",
    "        max_idx = min(len(column_names) - 1, scaled_features.shape[1] - 1)\n",
    "        dim1_idx = min(dim1_idx, max_idx)\n",
    "        dim2_idx = min(dim2_idx, max_idx)\n",
    "        \n",
    "        print(f\"Focusing on dimensions {dim1_idx+1} and {dim2_idx+1}\")\n",
    "        \n",
    "        # Create focused feature set\n",
    "        n_original_dims = len(column_names)\n",
    "        focused_features = scaled_features[:, :n_original_dims].copy()\n",
    "        \n",
    "        # Emphasize the specified adjacent dimensions\n",
    "        focused_features[:, dim1_idx] *= 3.0  # Strong emphasis\n",
    "        focused_features[:, dim2_idx] *= 3.0  # Strong emphasis\n",
    "        \n",
    "        # Add interaction between these dimensions\n",
    "        interaction_features = np.column_stack([\n",
    "            scaled_features[:, dim1_idx] * scaled_features[:, dim2_idx],  # Product\n",
    "            np.abs(scaled_features[:, dim1_idx] - scaled_features[:, dim2_idx]),  # Abs diff\n",
    "            (scaled_features[:, dim1_idx] + scaled_features[:, dim2_idx]) / 2  # Average\n",
    "        ])\n",
    "        \n",
    "        focused_features = np.column_stack([focused_features, interaction_features])\n",
    "        return focused_features\n",
    "    else:\n",
    "        return scaled_features\n",
    "\n",
    "def create_hybrid_features(scaled_features, column_names, adjacent_focus):\n",
    "    \"\"\"Create hybrid feature representation with multiple adjacent pairs\"\"\"\n",
    "    hybrid_features = scaled_features.copy()\n",
    "    \n",
    "    n_dims = len(column_names)\n",
    "    \n",
    "    # Add interactions for all adjacent pairs\n",
    "    interaction_features = []\n",
    "    for i in range(n_dims - 1):\n",
    "        dim1_vals = scaled_features[:, i]\n",
    "        dim2_vals = scaled_features[:, i + 1]\n",
    "        \n",
    "        # Weight based on which pair we're focusing on\n",
    "        weight = 1.0\n",
    "        if adjacent_focus and f\"dim{i+1}{i+2}\" == adjacent_focus:\n",
    "            weight = 2.0  # Give extra weight to the focused pair\n",
    "        \n",
    "        pair_interactions = np.column_stack([\n",
    "            dim1_vals * dim2_vals * weight,           # Product\n",
    "            np.abs(dim1_vals - dim2_vals) * weight,   # Absolute difference\n",
    "            (dim1_vals + dim2_vals) * weight / 2      # Average\n",
    "        ])\n",
    "        interaction_features.append(pair_interactions)\n",
    "    \n",
    "    # Combine all interaction features\n",
    "    if interaction_features:\n",
    "        all_interactions = np.column_stack(interaction_features)\n",
    "        hybrid_features = np.column_stack([hybrid_features, all_interactions])\n",
    "    \n",
    "    return hybrid_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a356eb21-3261-446b-b192-46bc545017b2",
    "_uuid": "a53a5ecb-c43d-4fbc-8faf-a8876825e68e",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### 2. Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "83d457e1-aee8-4239-90bb-5243559ee002",
    "_uuid": "2c681bf7-7781-4ec5-8d37-9db0b99d105d",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_specialized_gmm(k_clusters, covariance_type, features):\n",
    "    \"\"\"Create GMM specifically optimized for physics data with exponential distribution\"\"\"\n",
    "    \n",
    "    # Analyze feature characteristics to determine best GMM parameters\n",
    "    n_samples, n_features = features.shape\n",
    "    \n",
    "    # For exponential-like data, use more conservative regularization\n",
    "    reg_covar = 1e-4 if np.any(np.var(features, axis=0) < 0.1) else 1e-6\n",
    "    \n",
    "    # Adjust covariance type for physics data characteristics\n",
    "    if covariance_type == 'full' and n_features > n_samples // 10:\n",
    "        # Too many parameters for full covariance, use tied instead\n",
    "        covariance_type = 'tied'\n",
    "        print(f\"Adjusted covariance type to '{covariance_type}' for better numerical stability\")\n",
    "    \n",
    "    # Use more initialization attempts for exponential-like data\n",
    "    n_init = min(50, max(20, 100 // k_clusters))\n",
    "    \n",
    "    # Create specialized GMM\n",
    "    gmm = GaussianMixture(\n",
    "        n_components=k_clusters,\n",
    "        covariance_type=covariance_type,\n",
    "        init_params='k-means++',  # Better initialization for skewed data\n",
    "        n_init=n_init,\n",
    "        max_iter=2000,  # More iterations for convergence\n",
    "        tol=1e-7,  # Stricter tolerance\n",
    "        reg_covar=reg_covar,  # Adaptive regularization\n",
    "        random_state=42,\n",
    "        warm_start=False,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Pre-process: remove extreme outliers that might hurt GMM fitting\n",
    "    robust_scaler = RobustScaler()\n",
    "    features_robust = robust_scaler.fit_transform(features)\n",
    "    \n",
    "    # Detect extreme outliers (beyond 3 MAD)\n",
    "    median_abs_dev = np.median(np.abs(features_robust - np.median(features_robust, axis=0)), axis=0)\n",
    "    outlier_threshold = 3.0\n",
    "    extreme_outliers = np.any(np.abs(features_robust) > outlier_threshold, axis=1)\n",
    "    \n",
    "    if extreme_outliers.sum() > 0:\n",
    "        print(f\"Detected {extreme_outliers.sum()} extreme outliers for GMM preprocessing\")\n",
    "        # Fit on non-extreme data, then predict on all data\n",
    "        clean_features = features[~extreme_outliers]\n",
    "        if len(clean_features) > k_clusters * 10:  # Ensure enough data\n",
    "            print(\"Fitting GMM on cleaned data\")\n",
    "            # Create a wrapper that fits on clean data but predicts on all data\n",
    "            class CleanFitGMM:\n",
    "                def __init__(self, base_gmm, clean_mask, all_features):\n",
    "                    self.base_gmm = base_gmm\n",
    "                    self.clean_mask = clean_mask\n",
    "                    self.all_features = all_features\n",
    "                    self.fitted = False\n",
    "                \n",
    "                def fit(self, X):\n",
    "                    clean_X = X[~self.clean_mask]\n",
    "                    self.base_gmm.fit(clean_X)\n",
    "                    self.fitted = True\n",
    "                    return self\n",
    "                \n",
    "                def predict(self, X):\n",
    "                    if not self.fitted:\n",
    "                        raise ValueError(\"Model not fitted yet\")\n",
    "                    return self.base_gmm.predict(X)\n",
    "                \n",
    "                def fit_predict(self, X):\n",
    "                    self.fit(X)\n",
    "                    return self.predict(X)\n",
    "            \n",
    "            return CleanFitGMM(gmm, extreme_outliers, features)\n",
    "    \n",
    "    return gmm\n",
    "\n",
    "def create_specialized_gmm_v0(k_clusters, covariance_type, features):\n",
    "    \"\"\"Create GMM optimized for physics data\"\"\"\n",
    "    return GaussianMixture(\n",
    "        n_components=k_clusters,\n",
    "        covariance_type=covariance_type,\n",
    "        init_params='kmeans',\n",
    "        n_init=30,  # More initializations\n",
    "        max_iter=1000,  # More iterations\n",
    "        tol=1e-6,  # Stricter tolerance\n",
    "        reg_covar=1e-6,  # Regularization for numerical stability\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "def create_adjacent_focused_model(k_clusters, covariance_type):\n",
    "    \"\"\"Create model focused on adjacent dimension structure\"\"\"\n",
    "    # Use GMM with tied covariance (good for similar cluster shapes in physics)\n",
    "    return GaussianMixture(\n",
    "        n_components=k_clusters,\n",
    "        covariance_type=covariance_type,  # tied, Physics events often have similar covariance structure\n",
    "        init_params='k-means++',\n",
    "        n_init=60,  # More attempts for focused features\n",
    "        max_iter=1500,\n",
    "        tol=1e-7,\n",
    "        reg_covar=1e-5,  # Slightly more regularization for focused features\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "def create_hybrid_model(k_clusters, covariance_type):\n",
    "    \"\"\"Create hybrid model that considers outlier weights and adjacent structure\"\"\"\n",
    "    # For now, use standard GMM (sample_weight not available in sklearn GMM)\n",
    "    # Use full covariance for complex relationships in hybrid features\n",
    "    return GaussianMixture(\n",
    "        n_components=k_clusters,\n",
    "        covariance_type=covariance_type,  # More flexible for complex hybrid relationships\n",
    "        init_params='k-means++',\n",
    "        n_init=40,\n",
    "        max_iter=1200,\n",
    "        tol=1e-6,\n",
    "        reg_covar=1e-5,  # Balance between stability and flexibility\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "def create_gmm_model(k_clusters, covariance_type):\n",
    "    return GaussianMixture(n_components=k_clusters,\n",
    "        covariance_type=covariance_type,\n",
    "        init_params='kmeans',\n",
    "        n_init=20,\n",
    "        max_iter=500,\n",
    "        tol=1e-5,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "def create_kmeans_model(k_clusters):\n",
    "    return KMeans(n_clusters=k_clusters,\n",
    "        init='k-means++',\n",
    "        n_init=20,\n",
    "        max_iter=500,\n",
    "        random_state=42\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "30942e37-7b39-438e-86d5-d8647ca75988",
    "_uuid": "37415fdc-11aa-4c95-8cd1-4b9a69bd570d",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### 2.1 Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "078b9b3f-1a7a-4c6d-a645-0a792a3d3acd",
    "_uuid": "4c9fe9e9-b9e6-488a-8c0c-872014c59694",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def perform_hierarchical_clustering(k_clusters, adjacent_focus, features):\n",
    "    \"\"\"\n",
    "    Perform two-level hierarchical clustering to obtain exactly k_clusters clusters.\n",
    "    First level uses DBSCAN to separate major clusters based on selected dimensions.\n",
    "    Second level uses GMM to refine each major cluster.\n",
    "    \"\"\"\n",
    "    if adjacent_focus is None:\n",
    "        print(\"`adjacent_focus` not provided, defaulting to dimensions [1, 2] (Dim2 and Dim3).\")\n",
    "        dim_indices = [1, 2]\n",
    "    else:\n",
    "        try:\n",
    "            dims_str = re.findall(r'[1-9]', adjacent_focus)\n",
    "            if len(dims_str) < 2:\n",
    "                raise ValueError(\"Please provide at least two dimension digits, e.g., 'dim23' for Dim2 and Dim3.\")\n",
    "            dim_indices = [int(d) - 1 for d in dims_str[:2]]\n",
    "            print(f\"First-level clustering will focus on dimensions: {dim_indices}\")\n",
    "            \n",
    "        except (TypeError, ValueError) as e:\n",
    "            print(f\"Error parsing adjacent_focus ('{adjacent_focus}'). Falling back to KMeans. Error: {e}\")\n",
    "            \n",
    "            # Fallback: Use KMeans to ensure exactly k_clusters clusters\n",
    "            print(f\"Fallback strategy: Using KMeans to directly form {k_clusters} clusters\")\n",
    "            kmeans = KMeans(n_clusters=k_clusters, random_state=42, n_init=10)\n",
    "            return kmeans.fit_predict(features)\n",
    "    \n",
    "    if max(dim_indices) >= features.shape[1]:\n",
    "        print(\"Warning: Specified dimension index is out of range. Using fallback strategy - KMeans.\")\n",
    "        kmeans = KMeans(n_clusters=k_clusters, random_state=42, n_init=10)\n",
    "        return kmeans.fit_predict(features)\n",
    "    \n",
    "    first_level_features = features[:, dim_indices]\n",
    "    \n",
    "    # First-level clustering: Separate major clusters\n",
    "    print(\"Performing first-level clustering (major cluster separation)...\")\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=10)\n",
    "    main_clusters = dbscan.fit_predict(first_level_features)\n",
    "    \n",
    "    # Refine DBSCAN labels (assign noise points to nearest cluster)\n",
    "    unique_clusters = np.unique(main_clusters[main_clusters != -1])\n",
    "    if -1 in main_clusters:\n",
    "        noise_mask = (main_clusters == -1)\n",
    "        core_points = first_level_features[~noise_mask]\n",
    "        core_labels = main_clusters[~noise_mask]\n",
    "        if len(core_points) > 0:\n",
    "            for i in np.where(noise_mask)[0]:\n",
    "                dists = distance.cdist([first_level_features[i]], core_points)\n",
    "                closest_idx = np.argmin(dists)\n",
    "                main_clusters[i] = core_labels[closest_idx]\n",
    "        else:\n",
    "            print(\"Warning: DBSCAN found no core points. Using fallback strategy - KMeans.\")\n",
    "            n_fallback_clusters = max(2, min(k_clusters // 2, len(features) // 10))\n",
    "            kmeans = KMeans(n_clusters=n_fallback_clusters, random_state=42, n_init=10)\n",
    "            main_clusters = kmeans.fit_predict(first_level_features)\n",
    "    \n",
    "    n_main_clusters = len(np.unique(main_clusters))\n",
    "    if n_main_clusters == 0:\n",
    "        print(\"Error: No major clusters formed. Falling back to KMeans.\")\n",
    "        kmeans = KMeans(n_clusters=k_clusters, random_state=42, n_init=10)\n",
    "        return kmeans.fit_predict(features)\n",
    "    \n",
    "    print(f\"Identified {n_main_clusters} major clusters.\")\n",
    "    \n",
    "    # Second-level clustering: Refine each major cluster\n",
    "    print(\"Performing second-level clustering (refinement within major clusters)...\")\n",
    "    \n",
    "    cluster_sizes = []\n",
    "    cluster_indices = []\n",
    "    for cluster_idx in range(n_main_clusters):\n",
    "        cluster_mask = (main_clusters == cluster_idx)\n",
    "        cluster_size = np.sum(cluster_mask)\n",
    "        cluster_sizes.append(cluster_size)\n",
    "        cluster_indices.append(cluster_idx)\n",
    "    \n",
    "    subclusters_per_main = distribute_subclusters(cluster_sizes, k_clusters)\n",
    "    \n",
    "    final_labels = np.zeros(len(features), dtype=int)\n",
    "    current_label = 0\n",
    "    actual_clusters_created = 0\n",
    "    \n",
    "    for cluster_idx, n_subclusters in zip(cluster_indices, subclusters_per_main):\n",
    "        cluster_mask = (main_clusters == cluster_idx)\n",
    "        cluster_data = features[cluster_mask]\n",
    "        \n",
    "        if len(cluster_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        if n_subclusters <= 1 or len(cluster_data) < n_subclusters:\n",
    "            final_labels[cluster_mask] = current_label\n",
    "            actual_clusters_created += 1\n",
    "            current_label += 1\n",
    "        else:\n",
    "            try:\n",
    "                gmm = GaussianMixture(\n",
    "                    n_components=n_subclusters, \n",
    "                    covariance_type='tied',\n",
    "                    n_init=3, \n",
    "                    random_state=42,\n",
    "                    max_iter=100\n",
    "                )\n",
    "                sub_labels = gmm.fit_predict(cluster_data)\n",
    "                \n",
    "                unique_sub_labels = np.unique(sub_labels)\n",
    "                if len(unique_sub_labels) != n_subclusters:\n",
    "                    # GMM did not produce the expected number of clusters. Using fallback strategy - KMeans.\n",
    "                    kmeans_sub = KMeans(n_clusters=n_subclusters, random_state=42, n_init=10)\n",
    "                    sub_labels = kmeans_sub.fit_predict(cluster_data)\n",
    "                \n",
    "                final_labels[cluster_mask] = current_label + sub_labels\n",
    "                actual_clusters_created += n_subclusters\n",
    "                current_label += n_subclusters\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"GMM failed on cluster {cluster_idx}: {e}. Using fallback strategy - KMeans.\")\n",
    "                kmeans_sub = KMeans(n_clusters=n_subclusters, random_state=42, n_init=10)\n",
    "                sub_labels = kmeans_sub.fit_predict(cluster_data)\n",
    "                final_labels[cluster_mask] = current_label + sub_labels\n",
    "                actual_clusters_created += n_subclusters\n",
    "                current_label += n_subclusters\n",
    "    \n",
    "    unique_labels = np.unique(final_labels)\n",
    "    n_actual_clusters = len(unique_labels)\n",
    "    \n",
    "    print(f\"Actual clusters produced: {n_actual_clusters}, Target: {k_clusters}\")\n",
    "    \n",
    "    if n_actual_clusters < k_clusters:\n",
    "        print(f\"Warning: Fewer clusters ({n_actual_clusters}) than target ({k_clusters}). Adjusting...\")\n",
    "        final_labels = adjust_to_target_clusters(features, final_labels, k_clusters)\n",
    "    elif n_actual_clusters > k_clusters:\n",
    "        print(f\"Warning: More clusters ({n_actual_clusters}) than target ({k_clusters}). Merging...\")\n",
    "        final_labels = merge_excess_clusters(final_labels, k_clusters)\n",
    "    \n",
    "    # Ensure consecutive labeling\n",
    "    final_labels = pd.factorize(final_labels)[0]\n",
    "    \n",
    "    print(f\"Final number of clusters: {len(np.unique(final_labels))}\")\n",
    "    return final_labels\n",
    "\n",
    "def distribute_subclusters(cluster_sizes, k_clusters):\n",
    "    \"\"\"\n",
    "    Distribute subclusters among major clusters proportionally to reach k_clusters.\n",
    "    \"\"\"\n",
    "    n_main_clusters = len(cluster_sizes)\n",
    "    total_samples = sum(cluster_sizes)\n",
    "    \n",
    "    # Basic allocation: each main cluster gets at least one subcluster\n",
    "    subclusters_allocation = [1] * n_main_clusters\n",
    "    remaining_subclusters = k_clusters - n_main_clusters\n",
    "    \n",
    "    if remaining_subclusters > 0:\n",
    "        # Distribute remaining subclusters proportionally based on cluster sizes\n",
    "        cluster_proportions = [size / total_samples for size in cluster_sizes]\n",
    "        \n",
    "        # Calculate how many extra subclusters each cluster should get\n",
    "        extra_subclusters = [int(remaining_subclusters * prop) for prop in cluster_proportions]\n",
    "        \n",
    "        # Handle rounding remainder\n",
    "        assigned_extra = sum(extra_subclusters)\n",
    "        remaining_extra = remaining_subclusters - assigned_extra\n",
    "        \n",
    "        # Assign remaining subclusters to the largest clusters\n",
    "        if remaining_extra > 0:\n",
    "            sorted_indices = sorted(range(n_main_clusters), key=lambda i: cluster_sizes[i], reverse=True)\n",
    "            for i in range(remaining_extra):\n",
    "                extra_subclusters[sorted_indices[i]] += 1\n",
    "        \n",
    "        # Update final allocation\n",
    "        for i in range(n_main_clusters):\n",
    "            subclusters_allocation[i] += extra_subclusters[i]\n",
    "    \n",
    "    return subclusters_allocation\n",
    "\n",
    "def adjust_to_target_clusters(features, labels, k_clusters):\n",
    "    \"\"\"\n",
    "    When number of clusters is less than target, split the largest cluster further.\n",
    "    \"\"\"\n",
    "    current_k = len(np.unique(labels))\n",
    "    needed_clusters = k_clusters - current_k\n",
    "    \n",
    "    while needed_clusters > 0:\n",
    "        # Find the largest cluster to split\n",
    "        unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "        largest_cluster_idx = unique_labels[np.argmax(counts)]\n",
    "        largest_cluster_mask = (labels == largest_cluster_idx)\n",
    "        largest_cluster_data = features[largest_cluster_mask]\n",
    "        \n",
    "        if len(largest_cluster_data) < 2:\n",
    "            # Cannot split if cluster has fewer than 2 samples\n",
    "            break\n",
    "            \n",
    "        # Split the largest cluster into 2\n",
    "        kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "        sub_labels = kmeans.fit_predict(largest_cluster_data)\n",
    "        \n",
    "        # Update labels\n",
    "        max_label = np.max(labels)\n",
    "        labels[largest_cluster_mask] = np.where(sub_labels == 0, largest_cluster_idx, max_label + 1)\n",
    "        \n",
    "        needed_clusters -= 1\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def merge_excess_clusters(labels, k_clusters):\n",
    "    \"\"\"\n",
    "    When number of clusters exceeds target, merge the smallest clusters together.\n",
    "    \"\"\"\n",
    "    while len(np.unique(labels)) > k_clusters:\n",
    "        unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "        \n",
    "        # Identify the two smallest clusters to merge\n",
    "        sorted_indices = np.argsort(counts)\n",
    "        smallest_label = unique_labels[sorted_indices[0]]\n",
    "        second_smallest_label = unique_labels[sorted_indices[1]]\n",
    "        \n",
    "        # Merge the smallest cluster into the second smallest\n",
    "        labels[labels == smallest_label] = second_smallest_label\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fcdb028f-cf2b-451f-92b4-81ed2ed45eed",
    "_uuid": "385de925-6234-4bc2-a72a-9f7782dd5b82",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### 3. Post-processing - Refining clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6507c370-f96e-4394-ba64-f094e1e0917b",
    "_uuid": "225acbb6-896e-49b6-ab7f-6c5aa1ed2a0f",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def refine_clusters_physics_aware(cluster_labels, original_features):\n",
    "    \"\"\"Post-process clusters based on physics intuition\"\"\"\n",
    "    # Physics-based refinement: events with similar total energy should be in similar clusters\n",
    "    total_energy = np.sum(original_features.values, axis=1)\n",
    "    \n",
    "    # Simple refinement: ensure no cluster is too small\n",
    "    unique_labels, counts = np.unique(cluster_labels, return_counts=True)\n",
    "    min_cluster_size = max(5, len(cluster_labels) // (len(unique_labels) * 20))  # At least 5% of average\n",
    "    \n",
    "    for label, count in zip(unique_labels, counts):\n",
    "        if count < min_cluster_size:\n",
    "            # Reassign small clusters based on energy similarity\n",
    "            mask = cluster_labels == label\n",
    "            if mask.sum() > 0:\n",
    "                small_cluster_energies = total_energy[mask]\n",
    "                \n",
    "                # Find most similar cluster based on energy distribution\n",
    "                best_target_label = label\n",
    "                min_energy_diff = float('inf')\n",
    "                \n",
    "                for other_label in unique_labels:\n",
    "                    if other_label != label and np.sum(cluster_labels == other_label) >= min_cluster_size:\n",
    "                        other_energies = total_energy[cluster_labels == other_label]\n",
    "                        # Compare median energies\n",
    "                        energy_diff = abs(np.median(small_cluster_energies) - np.median(other_energies))\n",
    "                        if energy_diff < min_energy_diff:\n",
    "                            min_energy_diff = energy_diff\n",
    "                            best_target_label = other_label\n",
    "                \n",
    "                if best_target_label != label:\n",
    "                    print(f\"Reassigning small cluster {label} ({count} points) to cluster {best_target_label} based on energy similarity\")\n",
    "                    cluster_labels[mask] = best_target_label\n",
    "    \n",
    "    return cluster_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "789def65-a72b-424b-9f4e-27495615b32c",
    "_uuid": "15e9f900-6d43-4b34-a3ec-96fca15427b8",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c10c4071-0854-4274-9eaf-126dce04106b",
    "_uuid": "37a824db-22cd-4861-b129-c30d0f2d63bb",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # TODO: Upload the dataset, then set the path here\n",
    "    input_file = \"/kaggle/input/bda-final-dataset/public_data.csv\"\n",
    "    print(f\"dateset selected: {input_file}\")\n",
    "    \n",
    "    # Define all combinations\n",
    "    methods = ['kmeans', 'hierarchical', 'gmm', 'specialized_gmm', 'specialized_gmm_v0','adjacent_focus', 'hybrid']\n",
    "    scalers = ['standard', 'robust', 'quantile', 'power']\n",
    "    covariances = ['full', 'tied', 'diag', 'spherical']\n",
    "    flags = ['feature', 'outlier', 'refine']\n",
    "    \n",
    "    # Define adjacent focus options\n",
    "    n_dimensions = pd.read_csv(input_file).shape[1] - 1\n",
    "    adjacent_focuses = [None] + [f\"dim{i}{i+1}\" for i in range(1, n_dimensions)]\n",
    "\n",
    "    # TODO: choose your hyperparameters here !!!\n",
    "    methods = ['gmm']\n",
    "    scalers = ['power']\n",
    "    covariances = ['tied'] # choose 'full' if method = 'kmeans' or 'hierarchical', it actually means 'None'\n",
    "    adjacent_focuses = ['None'] # example: 'None', 'dim12', 'dim23', ...\n",
    "                                 # DO NOT choose 'None' if method2 = 'hierarchical' or 'adjacent_focus'\n",
    "    flags = []\n",
    "    print(f\"-> YOU CHOOSE method={methods}, scaler={scalers}, covariance={covariances}, adjacent={adjacent_focuses}, flag={flags}\")\n",
    "    \n",
    "    os.makedirs('result', exist_ok=True)\n",
    "    with open('config.txt', 'w') as f:\n",
    "        f.write(\"ID,Method,Scaler,Covariance,Adjacent_Focus,Time_Seconds\\n\")\n",
    "    \n",
    "    combination_id = 0\n",
    "    successful_combinations = 0\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "\n",
    "    for method in methods:\n",
    "        for scaler in scalers:\n",
    "            for covariance in covariances:\n",
    "                \n",
    "                # For non-GMM methods, use only 'full' covariance to avoid redundancy\n",
    "                if method not in ['gmm', 'specialized_gmm','specialized_gmm_v0', 'adjacent_focus', 'hybrid'] and covariance != 'full':\n",
    "                    continue\n",
    "                if method in [\"kmeans\", \"hierarchical\"]:\n",
    "                    covariance = None\n",
    "                \n",
    "                # For hierarchical, adjacent_focus and hybrid methods, try different dimension pairs\n",
    "                focus_options = adjacent_focuses if method in ['hierarchical', 'adjacent_focus', 'hybrid'] else [None]\n",
    "                \n",
    "                for adjacent_focus in focus_options:\n",
    "                    if method in ['hierarchical', 'adjacent_focus'] and adjacent_focus is None:\n",
    "                        continue\n",
    "                        \n",
    "                    combination_id += 1\n",
    "                    \n",
    "                    focus_str = adjacent_focus if adjacent_focus else 'None'\n",
    "                    cov_str = covariance if covariance else 'None'\n",
    "                    print(f\"\\n[ Combination {combination_id} ] {method} + {scaler} + {cov_str} + {focus_str}\")\n",
    "                    \n",
    "                    start_time = time.time()\n",
    "\n",
    "                    try:\n",
    "                        output_file = f\"result/submission_{combination_id}.csv\"\n",
    "                        \n",
    "                        perform_clustering(\n",
    "                            input_file, \n",
    "                            output_file,\n",
    "                            method=method,\n",
    "                            scaler_type=scaler,\n",
    "                            covariance_type=covariance,\n",
    "                            adjacent_focus=adjacent_focus,\n",
    "                            flags=flags\n",
    "                        )\n",
    "                        \n",
    "                        end_time = time.time()\n",
    "                        elapsed_time = end_time - start_time\n",
    "                        successful_combinations += 1\n",
    "                        \n",
    "                        print(f\"Runtime: {elapsed_time:.2f}s - SUCCESS\")\n",
    "                        \n",
    "                        with open('config.txt', 'a') as f:\n",
    "                            f.write(f\"{combination_id},{method},{scaler},{cov_str},{focus_str},{elapsed_time:.2f}\\n\")\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        end_time = time.time()\n",
    "                        elapsed_time = end_time - start_time\n",
    "                        print(f\"Runtime: {elapsed_time:.2f}s - FAILED: {str(e)}\")\n",
    "                        \n",
    "                        with open('config.txt', 'a') as f:\n",
    "                            f.write(f\"{combination_id},{method},{scaler},{cov_str},{focus_str},FAILED\\n\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"Finished! Total {combination_id} combinations, {successful_combinations} succeeded.\")\n",
    "    print(f\"\\nResults saved in 'result/' folder.\")\n",
    "    print(f\"Configuration log saved in 'config.txt'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a33bcd22-43e8-4b3a-8fbf-97a2c4713b67",
    "_uuid": "c421b6f6-b23b-4d89-b65d-bf5118faa0be",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.system(f\"zip -r results.zip result\")\n",
    "print(\"\\n Results saved as 'results.zip'\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7613277,
     "sourceId": 12093951,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
